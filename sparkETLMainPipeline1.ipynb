{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87515aaf-48d9-4a32-b145-c73e13f2eba6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inline function to reorder columns\n",
    "def reord_cols(df):\n",
    "    return df.select(\"id\", \"custprofession\", \"custage\", \"custlname\", \"custfname\")\n",
    "\n",
    "# Inline function to enrich data\n",
    "def enrich(df):\n",
    "    enrich_addcols_df6 = df.withColumn(\"curdt\", current_date()).withColumn(\"loadts\", current_timestamp())\n",
    "    enrich_ren_df7 = enrich_addcols_df6.withColumnRenamed(\"srcsystem\", \"src\")\n",
    "    enrich_combine_df8 = enrich_ren_df7.withColumn(\"nameprof\", concat(\"custfname\", lit(\" is a \"), \"custprofession\")).drop(\"custfname\")\n",
    "    enrich_combine_split_df9 = enrich_combine_df8.withColumn(\"custfname\", split(\"nameprof\", ' ')[0])\n",
    "    enrich_combine_split_cast_reformat_df10 = enrich_combine_split_df9.withColumn(\"custage\",regexp_replace(\"custage\",\"[-]\",'')).withColumn(\"curdtstr\", col(\"curdt\").cast(\"string\"))\\\n",
    "        .withColumn(\"year\", year(col(\"curdt\"))).withColumn(\"curdtstr\", concat(substring(\"curdtstr\", 3, 2), lit(\"/\"), substring(\"curdtstr\", 6, 2)))\\\n",
    "        .withColumn(\"dtfmt\", date_format(\"curdt\", 'yyyy/MM/dd hh:mm:ss'))\n",
    "    return enrich_combine_split_cast_reformat_df10\n",
    "\n",
    "# Inline function for pre-wrangling\n",
    "def pre_wrangle(df):\n",
    "    return df.select(\"id\", \"custprofession\", \"custage\", \"src\", \"curdt\")\\\n",
    "        .groupBy(\"custprofession\")\\\n",
    "        .agg(avg(\"custage\").alias(\"avgage\"))\\\n",
    "        .where(\"avgage>49\")\\\n",
    "        .orderBy(\"custprofession\")\n",
    "\n",
    "# Inline function for pre-wrangling analysis\n",
    "def prewrang_anal(df):\n",
    "    sample1 = df.sample(0.2, 10)\n",
    "    smry = df.summary()\n",
    "    coorval = df.corr(\"custage\", \"custage\")\n",
    "    covval = df.cov(\"custage\", \"custage\")\n",
    "    freqval = df.freqItems([\"custprofession\", \"agegroup\"], 0.4)\n",
    "    return sample1, smry, coorval, covval, freqval\n",
    "\n",
    "# Inline function to aggregate data\n",
    "def aggregate_data(df):\n",
    "    return df.groupby(\"year\", \"agegroup\", \"custprofession\").agg(max(\"curdt\").alias(\"max_curdt\"), min(\"curdt\").alias(\"min_curdt\"),\n",
    "                                                                 avg(\"custage\").alias(\"avg_custage\"), mean(\"custage\").alias(\"mean_age\"),\n",
    "                                                                 countDistinct(\"custage\").alias(\"distinct_cnt_age\"))\\\n",
    "             .orderBy(\"year\", \"agegroup\", \"custprofession\", ascending=[False, True, False])\n",
    "\n",
    "# Inline function to standardize columns\n",
    "def standardize_cols(df):\n",
    "    srcsys = 'Retail'\n",
    "    reord_added_df3 = df.withColumn(\"srcsystem\", lit(srcsys))\n",
    "    reord_added_replaced_df4 = reord_added_df3.withColumn(\"custfname\", col(\"custlname\"))\n",
    "    chgnumcol_reord_df5 = reord_added_replaced_df4.drop(\"custlname\")\n",
    "    return chgnumcol_reord_df5\n",
    "\n",
    "# Function to return a predefined schema\n",
    "def ret_struct():\n",
    "    return StructType([StructField(\"id\", IntegerType(), False),\n",
    "                       StructField(\"custfname\", StringType(), False),\n",
    "                       StructField(\"custlname\", StringType(), True),\n",
    "                       StructField(\"custage\", ShortType(), True),\n",
    "                       StructField(\"custprofession\", StringType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f4dd31f-931f-488f-8906-ddeb1c840c86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main(arg):\n",
    "    print(\"Define Spark session object (inline code)\")\n",
    "    spark = SparkSession.builder\\\n",
    "        .appName(\"Very Important SQL End to End App\")\\\n",
    "        .config(\"spark.jars\", \"dbfs:/FileStore/config/mysql_connector_java.jar\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    print(\"Set the logger level to error\")\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    print(\"1. Data Munging\")\n",
    "    custstructtype1 = ret_struct()\n",
    "    custdf_clean = spark.read.csv(arg[1], mode='dropmalformed', schema=custstructtype1)\n",
    "    custdf_optimized = custdf_clean.repartition(4).cache()\n",
    "\n",
    "    print(\"Dropping Duplicates of cust data\")\n",
    "    dedup_dropduplicates_df = custdf_optimized.dropDuplicates([\"custage\", \"id\"])\n",
    "    \n",
    "    txnsstructtype2 = StructType([StructField(\"txnid\", IntegerType(), False),\n",
    "                                  StructField(\"dt\", StringType()),\n",
    "                                  StructField(\"custid\", IntegerType()),\n",
    "                                  StructField(\"amt\", DoubleType()),\n",
    "                                  StructField(\"category\", StringType()),\n",
    "                                  StructField(\"product\", StringType()),\n",
    "                                  StructField(\"city\", StringType()),\n",
    "                                  StructField(\"state\", StringType()),\n",
    "                                  StructField(\"spendby\", StringType())])\n",
    "\n",
    "    txns = spark.read.csv(arg[2], mode='dropmalformed', schema=txnsstructtype2)\n",
    "    txns_clean_optimized = txns.repartition(1).cache()\n",
    "    \n",
    "    print(\"Dropping Duplicates of txns data\")\n",
    "    txns_dedup = txns_clean_optimized.dropDuplicates([\"dt\", \"amt\", \"txnid\"])\n",
    "\n",
    "    print(\"Data Preparation (Cleansing & Scrubbing)\")\n",
    "    prof_dict = {\"Therapist\": \"Physician\", \"Musician\": \"Music Director\", \"na\": \"prof not defined\"}\n",
    "    dedup_dropfillreplacena_clensed_scrubbed_df1 = dedup_dropduplicates_df.na.replace(prof_dict, subset=[\"custprofession\"])\n",
    "    dedup_dropfillreplacena_clensed_scrubbed_df1.show()\n",
    "\n",
    "    print(\"Data Standardization (column)\")\n",
    "    reord_df2 = reord_cols(dedup_dropfillreplacena_clensed_scrubbed_df1)\n",
    "    munged_df = standardize_cols(reord_df2)\n",
    "    munged_df.show()\n",
    "\n",
    "    print(\"Data Enrichment (values)\")\n",
    "    munged_enriched_df = enrich(munged_df)\n",
    "    munged_enriched_df.show()\n",
    "\n",
    "    print(\"Data Customization & Processing (Business logics)\")\n",
    "    from pyspark.sql.functions import udf\n",
    "    age_custom_validation = udf(lambda x: 'Adult' if x > 18 else 'Child')\n",
    "    custom_agegrp_munged_enriched_df = munged_enriched_df.withColumn(\"custage\",coalesce(\"custage\",lit(0)).cast(\"int\")).withColumn(\"agegroup\", age_custom_validation(\"custage\"))\n",
    "    custom_agegrp_munged_enriched_df.show()\n",
    "\n",
    "    print(\"Core Data Processing/Transformation (Level1) (Pre Wrangling) Curation\")\n",
    "    pre_wrangled_customized_munged_enriched_df = pre_wrangle(custom_agegrp_munged_enriched_df)\n",
    "    pre_wrangled_customized_munged_enriched_df.show()\n",
    "\n",
    "    filtered_nochildren_rowcol_df_for_further_wrangling1 = custom_agegrp_munged_enriched_df.filter(\"agegroup <> 'Child'\")\\\n",
    "        .select(\"id\", \"custage\", \"curdt\", \"custfname\", \"year\", \"agegroup\")\n",
    "    filtered_nochildren_rowcol_df_for_further_wrangling1.show()\n",
    "\n",
    "    dim_year_agegrp_prof_metrics_avg_mean_max_min_distinctCount_count_for_consumption2 = custom_agegrp_munged_enriched_df.filter(\"agegroup <> 'Child'\")\n",
    "    aggr_df = aggregate_data(dim_year_agegrp_prof_metrics_avg_mean_max_min_distinctCount_count_for_consumption2)\n",
    "    aggr_df.show()\n",
    "\n",
    "    aggr_filter_df = aggr_df.filter(\"avg_custage > 35\")\n",
    "    aggr_filter_df.show()\n",
    "\n",
    "    print(\"Analytical Functionalities\")\n",
    "    sampledf, summarydf, corrval, covval, freqdf = prewrang_anal(custom_agegrp_munged_enriched_df)\n",
    "    sampledf.show()\n",
    "    summarydf.show()\n",
    "    print(f\"Correlation value of age is {corrval}\")\n",
    "    print(f\"Covariance value of age is {covval}\")\n",
    "    freqdf.show()\n",
    "\n",
    "    masked_df = custom_agegrp_munged_enriched_df.withColumn(\"custfname\", md5(col(\"custfname\")))\n",
    "    masked_df.show()\n",
    "\n",
    "    print(\"Core Data Curation/Processing/Transformation (Level2) Data Wrangling\")\n",
    "    denormalizeddf = custdf_clean.alias(\"c\").join(txns_dedup.alias(\"t\"), col(\"c.id\") == col(\"t.custid\"), \"inner\")\n",
    "    denormalizeddf.show()\n",
    "    rno_txns3 = denormalizeddf.select(\"*\", row_number().over(Window.orderBy(\"dt\")).alias(\"sno\"))\n",
    "    rno_txns3.show()\n",
    "\n",
    "    print(\"Data Persistence & Consumption\")\n",
    "    print(\"final df output\")\n",
    "    custom_agegrp_munged_enriched_df.show()\n",
    "    custom_agegrp_munged_enriched_df.write.mode(\"overwrite\").saveAsTable(\"cust_final_tbl\")\n",
    "    aggr_filter_df.write.mode(\"overwrite\").json(\"/mnt/drive/ETLresult\")\n",
    "    #rno_txns3.write.format(\"jdbc\").option(\"url\", \"jdbc:mysql://localhost/custdb\").option(\"dbtable\", \"custtxns\")\\\n",
    "    #    .option(\"user\", \"root\").option(\"password\", \"password\").option(\"driver\", \"com.mysql.jdbc.Driver\").save()\n",
    "    \n",
    "    print(\"ETL Processing Completed\")\n",
    "    spark.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c208d03-abba-42c3-b538-951b172332f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"cust_data_path\", \"\")\n",
    "dbutils.widgets.text(\"txns_data_path\", \"\")\n",
    "dbutils.widgets.text(\"connection_properties_path\", \"\")\n",
    "\n",
    "input_path1 = dbutils.widgets.get(\"cust_data_path\")\n",
    "input_path2 = dbutils.widgets.get(\"txns_data_path\")\n",
    "connection_properties_path3 = dbutils.widgets.get(\"connection_properties_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8805ec7f-eb1b-437d-9733-d6959e289edd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.notebook.run(\"./reusable_functions\", 30,{\"param1\":\"- passed from the parent notebook\"})\n",
    "#dbutils.notebook.run(\"./reusable_functions\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58f53f56-8fe8-4262-8bd0-b153f56408ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.SparkStoppedException: Spark down: \n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:693)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:730)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:556)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:482)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:290)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached."
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.SparkStoppedException: Spark down: ",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:693)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:730)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:556)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:482)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:290)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import *\n",
    "    from pyspark.sql.types import *\n",
    "    from pyspark.sql.window import *\n",
    "    print(input_path1,input_path2,connection_properties_path3)\n",
    "    arg = [\"ETL Pipeline\",input_path1,input_path2,connection_properties_path3]\n",
    "    main(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a83a9da1-a699-47e0-9e5a-3c7987a6be9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sparkETLMainPipeline1",
   "widgets": {
    "connection_properties_path": {
     "currentValue": "dbfs:/FileStore/config/connection.prop",
     "nuid": "9eddb349-6057-4362-9280-653cb857ef89",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "connection_properties_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "cust_data_path": {
     "currentValue": "dbfs:/FileStore/data/custsmodified",
     "nuid": "d4c71ac7-79fa-4341-97cc-412e95a47bf0",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "cust_data_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "txns_data_path": {
     "currentValue": "dbfs:/FileStore/data/txns",
     "nuid": "910e3bc9-a955-4e04-be61-510d9032ac4d",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "txns_data_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
